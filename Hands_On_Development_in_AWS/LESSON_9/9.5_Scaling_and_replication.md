# 9.5 Scaling and replication 

# DynamoDB Scaling and Global Replication Demo

## Overview

This lesson demonstrates two key features of Amazon DynamoDB:
- **Autoscaling** of provisioned throughput
- **Global replication** using global tables

These are combined into a single demo due to the time-intensive nature of autoscaling. The demo is implemented in Python using the `app.py` script located in:

[app.py](./../CODE/DynamoDB/3_AutoScaling_Python/app.py)


---

## Prerequisites

- AWS credentials configured in the script (`access_key`, `secret_access_key`)
- Python environment with Boto3 installed
- Three JSON files:
  - `cars1.json` (1,500 items)
  - `cars2.json` (5,000 items)
  - `cars3.json` (5,000 items)

[3_AutoScaling_Python](./../CODE/DynamoDB/3_AutoScaling_Python)
---

## Part 1: Autoscaling Demo

### Step-by-Step Execution

1. **List Current Tables**
   - Confirm no existing tables in the Northern Virginia region.

2. **Create Table**
   - Table name: `carsPE`
   - Provisioned throughput: `5 RCUs / 5 WCUs`
   - Autoscaling: **Disabled**

3. **Load Data**
   - Load `cars1.json`, `cars2.json`, and `cars3.json` into memory.

4. **Insert Initial Items**
   - Insert 1,500 items from `cars1.json`
   - Monitor time taken (expected ~56 seconds)
   - Throughput remains at 5 WCUs

5. **Enable Autoscaling**
   - Go to AWS Console
   - Edit table settings:
     - Enable autoscaling for **write capacity**
     - Set max WCUs to `100`
     - Target utilization: `70%`
   - Save changes

6. **Insert Additional Items**
   - Insert 5,000 items from `cars2.json`
   - Wait 3 minutes
   - Insert 5,000 items from `cars3.json`
   - Compare insertion times to observe autoscaling behavior

---

## Part 2: Global Replication Demo

### Table Creation

1. **Create Table**
   - Name: `globalRepTable1`
   - Partition key: `ID` (String)
   - No sort key
   - Provisioned throughput: `1 RCU / 1 WCU`
   - Autoscaling: **Disabled initially**
   - Free tier eligible

2. **Enable Streams**
   - Required for global replication

3. **Enable Autoscaling**
   - Set max WCUs to `5`
   - Save changes

4. **Create Replica**
   - Region: `Frankfurt`
   - IAM role automatically created for cross-region access

---

## Verification

1. **Insert Item in Northern Virginia**
   - `ID: 1`
   - `f_name: Nick`

2. **Check Replication in Frankfurt**
   - Refresh table
   - Confirm item appears

3. **Multi-Master Rights**
   - Items can be created or deleted in either region
   - Changes propagate automatically

---

## Notes

- Autoscaling is now a prerequisite for global tables
- Streams are used for replication
- IAM roles are required for cross-service access
- Provisioned throughput can delay replication if not scaled

---

## Conclusion

This demo illustrates:
- How autoscaling affects write throughput over time
- How global tables replicate data across regions
- The importance of enabling autoscaling and streams for replication

# DynamoDB Scaling and Global Replication Demo (Continued)

## Part 3: Global Replication Verification

### Cross-Region Item Creation and Deletion

1. **Create Item in Frankfurt**
   - `ID: 2`
   - `f_name: Nick`

2. **Verify in Northern Virginia**
   - Refresh table
   - Confirm item replicated from Frankfurt

3. **Delete Item in Northern Virginia**
   - Item also removed from Frankfurt
   - Confirms **multi-master write capability**

---

## Part 4: Geographic DNS with Route 53

### Architecture Concept

Using **Amazon Route 53** with **Geographic DNS**, you can route users to region-specific infrastructure:

- **European Users**
  - Routed to Frankfurt-based web application
  - Access DynamoDB replica in Frankfurt

- **North American Users**
  - Routed to Northern Virginia-based infrastructure
  - Access DynamoDB replica in Northern Virginia

This setup ensures low latency and regional data locality while maintaining consistency via DynamoDB global tables.

---

## Part 5: Autoscaling Performance Analysis

### Item Count Verification

- Table: `carsPE`
- Live scan shows: `6,500 items`

### Timing Breakdown

| Batch         | Item Count | Time Taken | Notes                                  |
|---------------|------------|------------|----------------------------------------|
| Initial Insert| 1,500      | 56 sec     | Provisioned throughput: 5 WCUs         |
| First 5,000   | 5,000      | 274 sec    | Autoscaling enabled, ramping up        |
| Wait Period   | â€”          | 180 sec    | Waiting for autoscaling to take effect |
| Second 5,000  | 5,000      | 44 sec     | Autoscaling active, throughput scaled  |

### Observations

- **Autoscaling ramp-up time**: ~8 minutes
- **Final throughput**: Significantly improved
- **Optimization tips**:
  - Increase max WCUs beyond 100
  - Use **batch writes** (e.g., 25 items per request)

---

## Summary

This demo covered:

- **Autoscaling behavior**:
  - Initial delay before scaling
  - Significant performance boost after ramp-up

- **Global replication**:
  - Easy setup with streams and autoscaling
  - Multi-master writes across regions

- **Geographic routing**:
  - Route 53 enables region-aware traffic distribution
  - Supports scalable, low-latency global applications

---

## Recommendations

- Use **batch operations** for faster inserts
- Monitor autoscaling metrics in CloudWatch
- Consider **on-demand capacity mode** for unpredictable workloads
- Use **Route 53** for intelligent traffic routing across global replicas


 
 ## [Context](./../context.md)